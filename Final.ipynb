{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final\n",
    "## Name: Jeonseo David Lee (UNI: jl6569)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  From the perspective of a social scientist, which models did we learn this semester that are useful for ruling out alternative explanations through control variables AND that allow us to observe substantively meaningful information from model coefficients?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - One significant model covered this semester, employing control variables and providing insights into the individual contributions of distinct variables, is multiple regression. This model plays a crucial role in mitigating the impact of confounding variables, which are correlated with both independent and dependent variables, introducing potential spurious associations. By holding the effects of these variables constant, multiple regression aids in excluding alternative explanations, enabling a focused examination of the unique relationship and contribution of each variable in the model. This aspect holds substantial implications for interpretation, serving as a baseline value for the dependent variable when all independent variables are set to zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Describe the main differences between supervised and unsupervised learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - The main difference between supervised and unsupervised learning lies in the requirement for manually labeling outcome data. In supervised learning, features and their corresponding labels are provided, either through manual labeling or pre-existing data. For instance, the random forest classifier relies on labeled training sets where the target variable, such as \"spam\" or \"not,\" guides the model in learning patterns and relationships. This acquired knowledge enables predictions on new, unseen data.\n",
    "\n",
    "### - On the other hand, unsupervised learning comes into play when dealing with unlabeled data, autonomously uncovering patterns or relationships. It operates independently, discovering feature similarities through clustering without the need for human intervention in labeling target variables, as seen in supervised learning. Instead, unsupervised learning either groups similar instances through clustering or employs principal component analysis to reduce feature dimensions while retaining critical information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Is supervised or unsupervised learning the primary approach that is used by machine learning practitioners?  For whatever approach you think is secondary, why would you use this approach (what's a good reason to use these kinds of models?)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - The decision between employing supervised or unsupervised learning depends on the specific task and the characteristics of the available data. Both approaches are widely used in machine learning, and their prevalence depends on the application. Generally, neither approach is universally considered primary or secondary; instead, their importance varies based on the problem at hand.\n",
    "\n",
    "### - As mentioned earlier, supervised learning takes precedence when a distinct target variable requires prediction. In numerous real-world scenarios, tasks involve labeled data, and the goal is to achieve precise predictions or classifications.\n",
    "\n",
    "### - Conversely, unsupervised learning assumes a primary role when the task centers on exploring the inherent data structure, uncovering patterns, or clustering similar instances without explicit labels defined. Its utility becomes evident when labeled data is scarce, expensive, or simply unavailable. Hence, the adaptability of machine learning lies in its capacity to tailor the approach to the specific requirements of diverse tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Which unsupervised learning modeling approaches did we cover this semester?  What are the major differences between these techniques?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Unsupervised learning models covered in this semester include Principal Components Analysis (PCA), K-Means Clustering, and Hierarchical Clustering. PCA serves as a tool for dimensionality reduction and feature extraction without relying on labeled data. The resulting principal components are ranked based on the variance they explain within the dataset.\n",
    "\n",
    "### - K-Means Clustering's goal is to partition data into clusters by assigning points to clusters according to their proximity to centroids. The process iteratively refines cluster assignments until convergence, grouping similar instances together.\n",
    "\n",
    "### - Hierarchical Clustering constructs a cluster hierarchy, visualized as a dendrogram or tree structure. It either merges (agglomerative) or splits clusters based on proximity. Notably different from K-Means Clustering, hierarchical clustering introduces a layered structure, providing a detailed hierarchy of clusters, in contrast to the flat structure of distinct clusters seen in K-means clustering."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  What are the main benefits of using Principal Components Analysis?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - As previously mentioned, its main benefits lie in dimensionality reduction and feature extraction. This proves particularly beneficial when handling extensive, unlabeled datasets. Employing PCA allows us to condense the data into a select few components that effectively capture the variance within the dataset. This not only simplifies the data's complexity but also reduces training time for further machine learning models, serving as an essential preprocessing step. Additionally, PCA decorrelates features, diminishing sensitivity to multicollinearity and enhancing the overall stability of subsequent analyses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Thinking about neural networks, what are three major differences between a deep multilayer perceptron network and a convolutional neural network model?  Be sure to define any key terms in your explanation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### - The first difference between a deep multilayer perceptron network and a convolutional neural network model lies in their connectivity. While the deep multilayer perceptron network boasts full connectivity, linking each neuron in one layer to \"every\" neuron in the next layer, the CNN incorporates convolutional, pooling, and fully connected layers. This integration of layers shares weights, establishing a sparse connectivity pattern and effectively reducing the overall parameter count.\n",
    "\n",
    "### - Secondly, the approach to feature hierarchy based on weight sharing differs. In the case of a deep multilayer perceptron network, hierarchical features are not effectively captured, as it processes the entire input simultaneously. In contrast, a CNN excels in capturing hierarchical features, particularly through convolutional layers where lower layers identify simple features, building up to more complex ones.\n",
    "\n",
    "### - Lastly, spatial invariance sets them apart. The deep multilayer perceptron network lacks spatial invariance, making minor input changes result in significant output impacts. Conversely, a CNN achieves spatial invariance by employing convolutional and pooling layers, enabling pattern recognition irrespective of the precise spatial location.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Write the tf.keras code for a multilayer perceptron neural network with the following structure: Three hidden layers.  50 hidden units in the first hidden layer, 100 in the second, and 150 in the third.  Activate all hidden layers with relu.  The output layer should be built to classify to five categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Assume that we split the data into training/test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Build the Model: Set 3 hidden layers as 50, 100, and 150 (relu activation) and the output layer as 5 (softmax activation). \n",
    "\n",
    "model = Sequential([\n",
    "    Dense(50, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(150, activation='relu'),\n",
    "    Dense(5, activation='softmax')# There must be five categories in the output layer\n",
    "])\n",
    "\n",
    "sgd = SGD(learning_rate=0.0001) \n",
    "\n",
    "# 3. For multi-classification, complie by using Stochastic Gradient Descent (SGD) as the optimizer and categorical_crossentropy for loss\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 4. Train Model \n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# 5. Evaluate Model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Write the tf.keras code for a multilayer perceptron neural network with the following structure: Two hidden layers.  75 hidden units in the first hidden layer and 150 in the second.  Activate all hidden layers with relu.  The output layer should be built to classify a binary dependent variable.  Further, your optimization technique should be stochastic gradient descent. (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Assume that we split the data into training/test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Build the Model: Set 2 hidden layers as 75 and 150 (relu activation) and the output layer as 2 (sigmoid activation). \n",
    "\n",
    "model = Sequential([\n",
    "    Dense(75, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    Dense(150, activation='relu'),\n",
    "    Dense(2, activation='sigmoid')# There must be two categories in the output layer\n",
    "])\n",
    "\n",
    "sgd = SGD(learning_rate=0.0001) \n",
    "\n",
    "# 3. For binary classification, complie by using Stochastic Gradient Descent (SGD) as the optimizer and binary_crossentropy for loss\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 4. Train Model \n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# 5. Evaluate Model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.  Write the tf.keras code for a convolutional neural network with the following structure: Two convolutional layers.  16 filters in the first layer and 28 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  The output layer should be built to classify to ten categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build the model \n",
    "model = Sequential()\n",
    "\n",
    "# 2. First Convolutional Layer with 16 filters with customized (3,3) kernal size and 'relu' activation\n",
    "model.add(Conv2D(16, (3, 3),\n",
    "                 padding='valid',  activation = 'relu', input_shape=(width, height, channels)))\n",
    "\n",
    "# 3. First Pooling with a 2 by 2 filter \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 4. Second Convolutional Layer with 28 filters with customized (3,3) kernal size and 'relu' activation \n",
    "model.add(Conv2D(28, (3, 3),\n",
    "                 padding='valid',  activation = 'relu'))\n",
    "\n",
    "# 5. Second Pooling with a 2 by 2 filter \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 6. Flatten the output before the dense layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# 7. Output layer with 10 categories for multi-classification (assuming 10 categories) and softmax activation\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# 8. Lastly, compile the model using Stochastic Gradient Descent (SGD) as the optimizer and Categorical_Crossentropy for loss\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.  Write the keras code for a convolutional neural network with the following structure: Two convolutional layers.  32 filters in the first layer and 32 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  Add two fully connected layers with 128 hidden units in each layer and relu activations.  The output layer should be built to classify to six categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build the model \n",
    "model = Sequential()\n",
    "\n",
    "# 2. First Convolutional Layer with 32 filters with customized (3,3) kernal size and 'relu' activation\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, num_channels)))\n",
    "\n",
    "# 3. First Pooling with a 2 by 2 filter \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 4. Second Convolutional Layer with 32 filters with customized (3,3) kernal size and 'relu' activation \n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "\n",
    "# 5. Second Pooling with a 2 by 2 filter \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 6. Flatten the output to feed into the fully connected layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# 7. First fully connected layer with 128 hidden units and ReLU activation\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# 8. Second fully connected layer with 128 hidden units and ReLU activation\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# 9. Output layer with 6 categories for multi-classification (assuming 6 categories) and softmax activation\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# 10. Compile the model using Stochastic Gradient Descent (SGD) as the optimizer and Categorical_Crossentropy for loss\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
