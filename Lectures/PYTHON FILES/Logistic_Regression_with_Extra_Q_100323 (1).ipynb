{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"mELOi-lUIMF1"},"source":["# Logistic Regression"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":607,"status":"ok","timestamp":1696290859010,"user":{"displayName":"Charlie Riemann","userId":"03411403083236409064"},"user_tz":240},"id":"sG77NaXJIMF-","outputId":"649c58d5-ec59-424c-ab2c-67ee3ac095e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["   InMichelin Restaurant Name  Food  Decor  Service  Price\n","0           0  14 Wall Street    19     20       19     50\n","1           0             212    17     17       16     43\n","2           0        26 Seats    23     17       21     35\n","3           1              44    19     23       16     52\n","4           0               A    23     12       19     24\n"]}],"source":["#Import some example data\n","\n","import pandas as pd\n","# target = InMichelin, whether or not a restaurant is in the Michelin guide\n","data = pd.read_csv(\"http://gattonweb.uky.edu/sheather/book/docs/datasets/MichelinNY.csv\", encoding=\"latin_1\")\n","print(data.head())\n","\n","#update data to set up for train test split, remove Restaurant Name column\n","data = data.loc[:, data.columns != 'Restaurant Name']\n","y = data['InMichelin']\n","X = data.loc[:, data.columns != 'InMichelin']"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"M387P_MHIMGV"},"outputs":[{"name":"stdout","output_type":"stream","text":["logreg.coef_: [[ 0.38181614  0.07433425 -0.15691054  0.08189853]]\n","Training set score: 0.797\n","Test set score: 0.780\n","logreg.predict: [0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n"," 1 1 1 1]\n"]},{"name":"stderr","output_type":"stream","text":["/Users/jeonseo/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n","  warnings.warn(\n"]}],"source":["#Set up training and test data\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, random_state=42)\n","\n","#Note: random_state ensures same data will be generated for example each time\n","from sklearn.linear_model import LogisticRegression\n","\n","#set penalty to none since we are starting with non penalized logit, L1 and L2 are other options\n","logreg = LogisticRegression(penalty='none').fit(X_train, y_train)\n","\n","print(\"logreg.coef_: {}\".format(logreg .coef_))\n","\n","\n","print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n","print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n","#note - when doing model experimentation, fitting different algorithms you'd be doing cross validation with gridsearchcv or one at a time\n","\n","predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n","print(\"logreg.predict: {}\".format(predicted_vals))\n","#note - in sklearn predicted labels, not predicted probabilities, will be shown based on threshold of .5\n","#use predict_proba method to generate predicted probabilities"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"elapsed":247,"status":"ok","timestamp":1696292312893,"user":{"displayName":"Charlie Riemann","userId":"03411403083236409064"},"user_tz":240},"id":"Ycvi2emNIMGy","outputId":"d430d069-295b-4bd9-b28f-f1c6509289c0"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(penalty=&#x27;none&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(penalty=&#x27;none&#x27;)</pre></div></div></div></div></div>"],"text/plain":["LogisticRegression(penalty='none')"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["logreg\n","\n","#Use ?LogisticRegression() for more information"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fODA7tsxIMHI"},"source":["## Logistic Regression in statsmodels package"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353},"executionInfo":{"elapsed":1183,"status":"ok","timestamp":1696292327879,"user":{"displayName":"Charlie Riemann","userId":"03411403083236409064"},"user_tz":240},"id":"igNMntjKIMHN","outputId":"a5adce68-6784-4724-c77c-be1947476f8e"},"outputs":[{"data":{"text/html":["<table class=\"simpletable\">\n","<caption>Generalized Linear Model Regression Results</caption>\n","<tr>\n","  <th>Dep. Variable:</th>      <td>InMichelin</td>    <th>  No. Observations:  </th>  <td>   123</td> \n","</tr>\n","<tr>\n","  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   118</td> \n","</tr>\n","<tr>\n","  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     4</td> \n","</tr>\n","<tr>\n","  <th>Link Function:</th>         <td>Logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n","</tr>\n","<tr>\n","  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -57.266</td>\n","</tr>\n","<tr>\n","  <th>Date:</th>            <td>Tue, 03 Oct 2023</td> <th>  Deviance:          </th> <td>  114.53</td>\n","</tr>\n","<tr>\n","  <th>Time:</th>                <td>17:32:25</td>     <th>  Pearson chi2:      </th>  <td>  254.</td> \n","</tr>\n","<tr>\n","  <th>No. Iterations:</th>          <td>6</td>        <th>  Pseudo R-squ. (CS):</th>  <td>0.3534</td> \n","</tr>\n","<tr>\n","  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","     <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n","</tr>\n","<tr>\n","  <th>const</th>   <td>  -10.6490</td> <td>    2.588</td> <td>   -4.115</td> <td> 0.000</td> <td>  -15.722</td> <td>   -5.576</td>\n","</tr>\n","<tr>\n","  <th>Food</th>    <td>    0.3818</td> <td>    0.148</td> <td>    2.572</td> <td> 0.010</td> <td>    0.091</td> <td>    0.673</td>\n","</tr>\n","<tr>\n","  <th>Decor</th>   <td>    0.0743</td> <td>    0.103</td> <td>    0.720</td> <td> 0.471</td> <td>   -0.128</td> <td>    0.277</td>\n","</tr>\n","<tr>\n","  <th>Service</th> <td>   -0.1569</td> <td>    0.147</td> <td>   -1.070</td> <td> 0.285</td> <td>   -0.444</td> <td>    0.131</td>\n","</tr>\n","<tr>\n","  <th>Price</th>   <td>    0.0819</td> <td>    0.036</td> <td>    2.269</td> <td> 0.023</td> <td>    0.011</td> <td>    0.153</td>\n","</tr>\n","</table>"],"text/latex":["\\begin{center}\n","\\begin{tabular}{lclc}\n","\\toprule\n","\\textbf{Dep. Variable:}   &    InMichelin    & \\textbf{  No. Observations:  } &      123    \\\\\n","\\textbf{Model:}           &       GLM        & \\textbf{  Df Residuals:      } &      118    \\\\\n","\\textbf{Model Family:}    &     Binomial     & \\textbf{  Df Model:          } &        4    \\\\\n","\\textbf{Link Function:}   &      Logit       & \\textbf{  Scale:             } &    1.0000   \\\\\n","\\textbf{Method:}          &       IRLS       & \\textbf{  Log-Likelihood:    } &   -57.266   \\\\\n","\\textbf{Date:}            & Tue, 03 Oct 2023 & \\textbf{  Deviance:          } &    114.53   \\\\\n","\\textbf{Time:}            &     17:32:25     & \\textbf{  Pearson chi2:      } &     254.    \\\\\n","\\textbf{No. Iterations:}  &        6         & \\textbf{  Pseudo R-squ. (CS):} &   0.3534    \\\\\n","\\textbf{Covariance Type:} &    nonrobust     & \\textbf{                     } &             \\\\\n","\\bottomrule\n","\\end{tabular}\n","\\begin{tabular}{lcccccc}\n","                 & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n","\\midrule\n","\\textbf{const}   &     -10.6490  &        2.588     &    -4.115  &         0.000        &      -15.722    &       -5.576     \\\\\n","\\textbf{Food}    &       0.3818  &        0.148     &     2.572  &         0.010        &        0.091    &        0.673     \\\\\n","\\textbf{Decor}   &       0.0743  &        0.103     &     0.720  &         0.471        &       -0.128    &        0.277     \\\\\n","\\textbf{Service} &      -0.1569  &        0.147     &    -1.070  &         0.285        &       -0.444    &        0.131     \\\\\n","\\textbf{Price}   &       0.0819  &        0.036     &     2.269  &         0.023        &        0.011    &        0.153     \\\\\n","\\bottomrule\n","\\end{tabular}\n","%\\caption{Generalized Linear Model Regression Results}\n","\\end{center}"],"text/plain":["<class 'statsmodels.iolib.summary.Summary'>\n","\"\"\"\n","                 Generalized Linear Model Regression Results                  \n","==============================================================================\n","Dep. Variable:             InMichelin   No. Observations:                  123\n","Model:                            GLM   Df Residuals:                      118\n","Model Family:                Binomial   Df Model:                            4\n","Link Function:                  Logit   Scale:                          1.0000\n","Method:                          IRLS   Log-Likelihood:                -57.266\n","Date:                Tue, 03 Oct 2023   Deviance:                       114.53\n","Time:                        17:32:25   Pearson chi2:                     254.\n","No. Iterations:                     6   Pseudo R-squ. (CS):             0.3534\n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          z      P>|z|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const        -10.6490      2.588     -4.115      0.000     -15.722      -5.576\n","Food           0.3818      0.148      2.572      0.010       0.091       0.673\n","Decor          0.0743      0.103      0.720      0.471      -0.128       0.277\n","Service       -0.1569      0.147     -1.070      0.285      -0.444       0.131\n","Price          0.0819      0.036      2.269      0.023       0.011       0.153\n","==============================================================================\n","\"\"\""]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import statsmodels.api as sm\n","#remember still need to add column of 1's\n","X_train_new = sm.add_constant(X_train)\n","\n","#remember that y is first and then X in statsmodel; Generalized Linear Model and binomial family for Logistic regression\n","model = sm.GLM(y_train, X_train_new, family=sm.families.Binomial()).fit()\n","\n","model.summary()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8_VyFy5-IMHc"},"source":["## Logistic Regression with constraints on size of coefficients"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":279,"status":"ok","timestamp":1696292370885,"user":{"displayName":"Charlie Riemann","userId":"03411403083236409064"},"user_tz":240},"id":"7kW-yfnUIMHf","outputId":"3ce16b09-32b0-4292-d44c-f99f2ceef7d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["logreg .coef_: [[ 0.38171368  0.07433904 -0.15682846  0.08189077]]\n","Training set score: 0.797\n","Test set score: 0.780\n","logreg.predict: [0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n"," 1 1 1 1]\n"]}],"source":["# Smaller C will constrain Betas more.  It's a tuning parameter we can find using gridsearch.\n","# Lowering C will make coefficients larger, larger C will make coefficients smaller\n","# Note: L2 (Ridge) will shrink coefficients down, never reaching 0. L1 (Lasso) has potential to zero out coefficients\n","\n","#C=100, compare coefs to regular model above.\n","logreg = LogisticRegression(C=100, penalty='l2').fit(X_train, y_train)\n","\n","print(\"logreg .coef_: {}\".format(logreg .coef_))\n","\n","\n","print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n","print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n","\n","\n","predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n","print(\"logreg.predict: {}\".format(predicted_vals))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1696292384047,"user":{"displayName":"Charlie Riemann","userId":"03411403083236409064"},"user_tz":240},"id":"4RCMm5WCIMHq","outputId":"4c82663c-7362-4580-af45-8a07002ebb8a"},"outputs":[{"name":"stdout","output_type":"stream","text":["logreg .coef_: [[ 0.37187726  0.07490079 -0.14897911  0.08113593]]\n","Training set score: 0.797\n","Test set score: 0.780\n","logreg.predict: [0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n"," 1 1 1 1]\n"]}],"source":["#Now change to C=1, compare coefs to above models.\n","logreg = LogisticRegression(C=1, penalty='l2').fit(X_train, y_train)\n","\n","print(\"logreg .coef_: {}\".format(logreg .coef_))\n","\n","\n","print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n","print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n","\n","\n","predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n","print(\"logreg.predict: {}\".format(predicted_vals))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243,"status":"ok","timestamp":1696292409239,"user":{"displayName":"Charlie Riemann","userId":"03411403083236409064"},"user_tz":240},"id":"DpGBOiXCIMIf","outputId":"c9b9220a-7e2c-4067-dde2-f68f1cb770cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["logreg .coef_: [[0.00549429 0.00672568 0.00502413 0.02866617]]\n","Training set score: 0.699\n","Test set score: 0.732\n","logreg.predict: [0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n"," 1 1 0 0]\n"]}],"source":["\n","#Now make C even smaller.  Set C=.0001, compare coefs to above models.\n","\n","#Does the model's prediction power get better or worse??\n","\n","logreg = LogisticRegression(C=.0001, penalty='l2').fit(X_train, y_train)\n","\n","print(\"logreg .coef_: {}\".format(logreg .coef_))\n","\n","\n","print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n","print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n","\n","\n","predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n","print(\"logreg.predict: {}\".format(predicted_vals))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":152,"status":"ok","timestamp":1696295998666,"user":{"displayName":"Charlie Riemann","userId":"03411403083236409064"},"user_tz":240},"id":"QJlOnuHVnZJd","outputId":"f3c8301f-7275-47a8-c8a9-8400ca972adf"},"outputs":[{"name":"stdout","output_type":"stream","text":["logreg .coef_: [[-0.02290063  0.          0.          0.00967784]]\n","Training set score: 0.699\n","Test set score: 0.732\n","logreg.predict: [[0.50869302 0.49130698]\n"," [0.52166386 0.47833614]\n"," [0.50562721 0.49437279]\n"," [0.49796135 0.50203865]\n"," [0.4941774  0.5058226 ]\n"," [0.56184299 0.43815701]\n"," [0.50474107 0.49525893]\n"," [0.52737519 0.47262481]\n"," [0.49353051 0.50646949]\n"," [0.56596642 0.43403358]\n"," [0.52584631 0.47415369]\n"," [0.50562721 0.49437279]\n"," [0.529142   0.470858  ]\n"," [0.4858678  0.5141322 ]\n"," [0.44997926 0.55002074]\n"," [0.50869302 0.49130698]\n"," [0.50143551 0.49856449]\n"," [0.56097012 0.43902988]\n"," [0.49990229 0.50009771]\n"," [0.48256541 0.51743459]\n"," [0.53243515 0.46756485]\n"," [0.52254837 0.47745163]\n"," [0.43367605 0.56632395]\n"," [0.51288298 0.48711702]\n"," [0.50627411 0.49372589]\n"," [0.49659665 0.50340335]\n"," [0.49990229 0.50009771]\n"," [0.51046484 0.48953516]\n"," [0.51683215 0.48316785]\n"," [0.4858678  0.5141322 ]\n"," [0.51618587 0.48381413]\n"," [0.4858678  0.5141322 ]\n"," [0.47991044 0.52008956]\n"," [0.53331756 0.46668244]\n"," [0.54685961 0.45314039]\n"," [0.49506353 0.50493647]\n"," [0.50167476 0.49832524]\n"," [0.43478206 0.56521794]\n"," [0.48893229 0.51106771]\n"," [0.48716083 0.51283917]\n"," [0.49199761 0.50800239]]\n"]}],"source":["#What if we want to use an l1 penalty instead?  Change penalty to 'l1' and solver to 'liblinear'.\n","\n","#Does the model's prediction power get better or worse?? Do any coefficients shrink to 0 and drop out of model?\n","# Note: this can be helpful to understand feature importance for additional research\n","# Solvers are used to optimize parameters of the model.Liblinear is commonly used solver that handles both L1 and L2.\n","\n","logreg = LogisticRegression(C=.01, penalty='l1',solver='liblinear').fit(X_train, y_train)\n","\n","print(\"logreg .coef_: {}\".format(logreg .coef_))\n","\n","\n","print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n","print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n","\n","\n","predicted_vals = logreg.predict_proba(X_test) # y_pred includes your predictions\n","print(\"logreg.predict: {}\".format(predicted_vals))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FT3rYS9jIMIn"},"source":["## Multiclass models (Multinomial model) - Where Dependent Variables are more than TWO!"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R1JVt7blIMIp","outputId":"e5136842-c88f-470d-a55e-c103edc04c0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n"]}],"source":["from sklearn.datasets import load_iris\n","import numpy as np\n","\n","##Three categories for the dependent variable - different iris flower types - disetosa, versicolor, virginica\n","iris = load_iris()\n","iris\n","X, y = iris.data, iris.target\n","\n","print(iris.feature_names )# X variable names\n","print(X[0:5]) # first five rows of data\n","\n","print(iris.target_names) #target categories\n","print(np.unique(y)) #target values\n","\n","# print(iris.keys()) - load gives you a didctionary, not a dataframe \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KuMM3GkSIMIv"},"outputs":[],"source":["logreg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\",max_iter=10000).fit(X,y)\n","#Note that model is being fit to entire dataset X,y but you should always be using some form of cross validation to find the best model\n","#Note the three argument changes to LogisticRegression()  Need to set to multinomial, change algorithm to lbfgs for iterating for multinomial, increase iterations to make sure find best coefficients"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1696195925276,"user":{"displayName":"Charlie Riemann","userId":"03411403083236409064"},"user_tz":240},"id":"eAjgz06fIMI3","outputId":"5c2a37be-d281-4fa7-ab8c-f1997ff4d7df"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1\n"," 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 1 1 0 1 0\n"," 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0\n"," 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1\n"," 1 0 1 1 0 1 0 0 0 0 0 1 1 1 0 1]\n"]}],"source":["print(logreg.predict(X)) #run predicted probabilities through softmax function to predict categories for new X data, but I am being lazy and using X data here."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Softmax"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":373,"status":"ok","timestamp":1696195894055,"user":{"displayName":"Charlie Riemann","userId":"03411403083236409064"},"user_tz":240},"id":"037ICwrGIMI_","outputId":"2b5c3d1a-ea15-4606-9402-0cfd6b6e3cf1"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.09003057317038046 0.6652409557748219 0.24472847105479767\n","1.0\n"]}],"source":["# Softmax used for predicted probabilities.Here's how Softmax is calculated with some example data...\n","# transform values into probabilities\n","# for now we use Softmax to help extend into multinomial logit, but will use towards end of class for neural networks\n","from math import exp\n","# calculate each probability\n","p1 = exp(1) / (exp(1) + exp(3) + exp(2))\n","p2 = exp(3) / (exp(1) + exp(3) + exp(2))\n","p3 = exp(2) / (exp(1) + exp(3) + exp(2))\n","# report probabilities\n","print(p1, p2, p3)\n","# report sum of probabilities\n","print(p1 + p2 + p3)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ekb9MV5dg-1k"},"source":["# Extra Practice for Logit Regression\n","##Theory Questions 1-5\n","##Coding Questions 6-9"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TciVKOk3hVsh"},"source":["###Question 1:\n","What is logistic regression, and how does it differ from linear regression?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- Logistic Regression gives us the probability that lies between 0 and 1 (while linear gives us the numeric value)\n","- Logistic regression is used for binary outcomes (classification!)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tn2xk506heDJ"},"source":["###Question 2:\n","Describe the process of splitting a dataset into training and testing sets for logistic regression?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- 1) Define X and Y (Dependent) \n","- 2) We then split them into 80-20 (customized on your end)\n","- 3) Fit those train data into logistic regression model "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"n4FzwqTzhokI"},"source":["###Question 3:\n","What is regularization in logistic regression, and why is it important?"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"v2LR8nb3iIsi"},"source":["###Question 4:\n","Walk me through the process of fitting a logistic regression model in Python using a library scikit-learn."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8ICyRHK8iPC2"},"source":["###Question 5:\n","Explain the process of hyperparameter tuning for a logistic regression model."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ziv7Gv-vmQn_"},"source":["###Question 6-9:\n","In this series of questions you will be asked to find the optimal hyperparameters for a dataset of your choosing and running a logistic regression model, including the regularization strength (C) and the penalty type (L1 or L2). Each question tests a differst part of what makes up the optimal logistic regression model for this dataset. In my question and answer I chose to do it on the Iris Dataset."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"PtaVsufwonUa"},"source":["###Question 6:\n","Tune the regulation strength (C) of the model. Hint: list out the possible values of C, such that [0.001, 0.01, 0.1, 1. etc...]. Make sure to import GridSearchCV"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"oDZliGl_pT1j"},"source":["###Question 7:\n","Now set up a hyperparamter grid where you explore the two penalty types for the model [L1 & L2] along with liblinear solver."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9903bL-9sGY9"},"source":["##Question 8:\n"," Now set up and execute a Gridsearch for the best hyperparameters, fit the best model, and retrieve the optimal hyperparameters using scikit-learn."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lnDRrhq3nMxf"},"source":["###Question 9:\n","Print the results stating which is the best logistic regression model, traning set scire, test set score, and the best logistic regression prediction."]}],"metadata":{"colab":{"provenance":[{"file_id":"1rLsgA1lEWQBdRJEwRkC2wl75iRrq2YCR","timestamp":1696300936759}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
