{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"nq0FojiiIeOw"},"source":["# Linear Regression"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2At-4G-FIeO2"},"source":["## Import Data"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":122,"status":"ok","timestamp":1695670987752,"user":{"displayName":"Charlie Riemann","userId":"03411403083236409064"},"user_tz":240},"id":"t23Egr9oIeO7"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":154,"status":"ok","timestamp":1695670988109,"user":{"displayName":"Charlie Riemann","userId":"03411403083236409064"},"user_tz":240},"id":"VMXDT3tZIePS","outputId":"af74eeac-3274-4b4d-f067-d3ea0ec0e840"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>rownames</th>\n","      <th>crim</th>\n","      <th>zn</th>\n","      <th>indus</th>\n","      <th>chas</th>\n","      <th>nox</th>\n","      <th>rm</th>\n","      <th>age</th>\n","      <th>dis</th>\n","      <th>rad</th>\n","      <th>tax</th>\n","      <th>ptratio</th>\n","      <th>black</th>\n","      <th>lstat</th>\n","      <th>medv</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0.00632</td>\n","      <td>18.0</td>\n","      <td>2.31</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>6.575</td>\n","      <td>65.2</td>\n","      <td>4.0900</td>\n","      <td>1</td>\n","      <td>296</td>\n","      <td>15.3</td>\n","      <td>396.90</td>\n","      <td>4.98</td>\n","      <td>24.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0.02731</td>\n","      <td>0.0</td>\n","      <td>7.07</td>\n","      <td>0</td>\n","      <td>0.469</td>\n","      <td>6.421</td>\n","      <td>78.9</td>\n","      <td>4.9671</td>\n","      <td>2</td>\n","      <td>242</td>\n","      <td>17.8</td>\n","      <td>396.90</td>\n","      <td>9.14</td>\n","      <td>21.6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0.02729</td>\n","      <td>0.0</td>\n","      <td>7.07</td>\n","      <td>0</td>\n","      <td>0.469</td>\n","      <td>7.185</td>\n","      <td>61.1</td>\n","      <td>4.9671</td>\n","      <td>2</td>\n","      <td>242</td>\n","      <td>17.8</td>\n","      <td>392.83</td>\n","      <td>4.03</td>\n","      <td>34.7</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0.03237</td>\n","      <td>0.0</td>\n","      <td>2.18</td>\n","      <td>0</td>\n","      <td>0.458</td>\n","      <td>6.998</td>\n","      <td>45.8</td>\n","      <td>6.0622</td>\n","      <td>3</td>\n","      <td>222</td>\n","      <td>18.7</td>\n","      <td>394.63</td>\n","      <td>2.94</td>\n","      <td>33.4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0.06905</td>\n","      <td>0.0</td>\n","      <td>2.18</td>\n","      <td>0</td>\n","      <td>0.458</td>\n","      <td>7.147</td>\n","      <td>54.2</td>\n","      <td>6.0622</td>\n","      <td>3</td>\n","      <td>222</td>\n","      <td>18.7</td>\n","      <td>396.90</td>\n","      <td>5.33</td>\n","      <td>36.2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   rownames     crim    zn  indus  chas    nox     rm   age     dis  rad  tax  \\\n","0         1  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296   \n","1         2  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242   \n","2         3  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242   \n","3         4  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222   \n","4         5  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222   \n","\n","   ptratio   black  lstat  medv  \n","0     15.3  396.90   4.98  24.0  \n","1     17.8  396.90   9.14  21.6  \n","2     17.8  392.83   4.03  34.7  \n","3     18.7  394.63   2.94  33.4  \n","4     18.7  396.90   5.33  36.2  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["\n","data = pd.read_csv(\"Boston.csv\")\n","data.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":310},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1695670988110,"user":{"displayName":"Charlie Riemann","userId":"03411403083236409064"},"user_tz":240},"id":"3WmIe-4PIePh","outputId":"4b6d6c8a-d5f9-4cd3-bd64-f04e802aefec"},"outputs":[{"name":"stdout","output_type":"stream","text":["0    24.0\n","1    21.6\n","2    34.7\n","3    33.4\n","4    36.2\n","Name: medv, dtype: float64\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>zn</th>\n","      <th>indus</th>\n","      <th>chas</th>\n","      <th>nox</th>\n","      <th>rm</th>\n","      <th>age</th>\n","      <th>dis</th>\n","      <th>rad</th>\n","      <th>tax</th>\n","      <th>ptratio</th>\n","      <th>black</th>\n","      <th>lstat</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>18.0</td>\n","      <td>2.31</td>\n","      <td>0</td>\n","      <td>0.538</td>\n","      <td>6.575</td>\n","      <td>65.2</td>\n","      <td>4.0900</td>\n","      <td>1</td>\n","      <td>296</td>\n","      <td>15.3</td>\n","      <td>396.90</td>\n","      <td>4.98</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>7.07</td>\n","      <td>0</td>\n","      <td>0.469</td>\n","      <td>6.421</td>\n","      <td>78.9</td>\n","      <td>4.9671</td>\n","      <td>2</td>\n","      <td>242</td>\n","      <td>17.8</td>\n","      <td>396.90</td>\n","      <td>9.14</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>7.07</td>\n","      <td>0</td>\n","      <td>0.469</td>\n","      <td>7.185</td>\n","      <td>61.1</td>\n","      <td>4.9671</td>\n","      <td>2</td>\n","      <td>242</td>\n","      <td>17.8</td>\n","      <td>392.83</td>\n","      <td>4.03</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>2.18</td>\n","      <td>0</td>\n","      <td>0.458</td>\n","      <td>6.998</td>\n","      <td>45.8</td>\n","      <td>6.0622</td>\n","      <td>3</td>\n","      <td>222</td>\n","      <td>18.7</td>\n","      <td>394.63</td>\n","      <td>2.94</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>2.18</td>\n","      <td>0</td>\n","      <td>0.458</td>\n","      <td>7.147</td>\n","      <td>54.2</td>\n","      <td>6.0622</td>\n","      <td>3</td>\n","      <td>222</td>\n","      <td>18.7</td>\n","      <td>396.90</td>\n","      <td>5.33</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio   black  \\\n","0  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3  396.90   \n","1   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8  396.90   \n","2   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8  392.83   \n","3   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7  394.63   \n","4   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7  396.90   \n","\n","   lstat  \n","0   4.98  \n","1   9.14  \n","2   4.03  \n","3   2.94  \n","4   5.33  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["#clean up data and prep for linear regression\n","\n","data = data.iloc[:, 1:] #delete first column\n","\n","y = data['medv'] # median value of homes\n","X = data.loc[:, data.columns != 'medv']  #keep all columns except for median value column\n","\n","\n","\n","print(y[0:5])\n","X.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"I4mx_qRZIePw"},"source":["## Create training and test data for model evaluation"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1695670988111,"user":{"displayName":"Charlie Riemann","userId":"03411403083236409064"},"user_tz":240},"id":"O1RFKwyIIePy","outputId":"b5ef80d4-2d85-47e6-aae1-823998431dbb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',\n","       'ptratio', 'black', 'lstat'],\n","      dtype='object')\n","lr.coef_: [-1.28322638e-01  2.95517751e-02  4.88590934e-02  2.77350326e+00\n"," -1.62388292e+01  4.36875476e+00 -9.24808158e-03 -1.40086668e+00\n","  2.57761243e-01 -9.95694820e-03 -9.23122944e-01  1.31854199e-02\n"," -5.17639519e-01]\n","lr.intercept_: 29.836420163839335\n"]}],"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","\n","\n","lr = LinearRegression()\n","lr.fit(X_train, y_train)\n","\n","#The “slope” parameters (w), also called weights or coefficients, are stored in the coef_\n","#..attribute, while the offset or intercept (b) is stored in the intercept_ attribute:\n","\n","print(X_train.columns)  #column names to help identify output\n","print(\"lr.coef_: \"+str(lr.coef_))  #combine some text with a vector of beta coefficients that are converted to string data to enable it to be printed\n","print(\"lr.intercept_: {}\".format(lr.intercept_))  #combine some text and the intercept"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1695670988111,"user":{"displayName":"Charlie Riemann","userId":"03411403083236409064"},"user_tz":240},"id":"RhkEpH3UIeQA","outputId":"39c966e1-ae69-4f32-becd-27eb95e09d97"},"outputs":[{"data":{"text/plain":["['__abstractmethods__',\n"," '__annotations__',\n"," '__class__',\n"," '__delattr__',\n"," '__dict__',\n"," '__dir__',\n"," '__doc__',\n"," '__eq__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__getstate__',\n"," '__gt__',\n"," '__hash__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__le__',\n"," '__lt__',\n"," '__module__',\n"," '__ne__',\n"," '__new__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__setattr__',\n"," '__setstate__',\n"," '__sizeof__',\n"," '__str__',\n"," '__subclasshook__',\n"," '__weakref__',\n"," '_abc_impl',\n"," '_check_feature_names',\n"," '_check_n_features',\n"," '_decision_function',\n"," '_estimator_type',\n"," '_get_param_names',\n"," '_get_tags',\n"," '_more_tags',\n"," '_parameter_constraints',\n"," '_repr_html_',\n"," '_repr_html_inner',\n"," '_repr_mimebundle_',\n"," '_set_intercept',\n"," '_validate_data',\n"," '_validate_params',\n"," 'coef_',\n"," 'copy_X',\n"," 'feature_names_in_',\n"," 'fit',\n"," 'fit_intercept',\n"," 'get_params',\n"," 'intercept_',\n"," 'n_features_in_',\n"," 'n_jobs',\n"," 'positive',\n"," 'predict',\n"," 'rank_',\n"," 'score',\n"," 'set_params',\n"," 'singular_']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["dir(lr) #Use dir() to get a directory of the attributes and methods you can refer to for the linear regression object"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1695670988111,"user":{"displayName":"Charlie Riemann","userId":"03411403083236409064"},"user_tz":240},"id":"YU2t0bypIeQO","outputId":"66f2d922-79ed-4597-fc19-69224fd9fa96"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set score: 0.75\n","Test set score: 0.68\n","0.7160133196648376\n","Linear's MSE: 22.098694827098036\n"]}],"source":["# Let’s look at the training set and test set performance using r squared:\n","\n","print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n","print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\n","\n","#cross validation - default is k fold, specify 10 folds and make sure scoring is based on r2\n","from sklearn.model_selection import cross_val_score\n","\n","print(np.mean(cross_val_score(LinearRegression(), X_train, y_train, cv=10, scoring=\"r2\")))   #print mean of the 10 values of r2 that are generated\n","\n","\n","# MSE\n","y_pred = lr.predict(X_test)\n","from sklearn.metrics import mean_squared_error\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"Linear's MSE: {mse}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0E_NyEjjIeQl"},"source":["## Now let's try it with the statsmodels library"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":732},"executionInfo":{"elapsed":132,"status":"ok","timestamp":1695670988233,"user":{"displayName":"Charlie Riemann","userId":"03411403083236409064"},"user_tz":240},"id":"iGHZrpSvIeQn","outputId":"d30f0ee9-22f1-4fdf-cf56-a9cb5a9a086d"},"outputs":[{"data":{"text/html":["<table class=\"simpletable\">\n","<caption>OLS Regression Results</caption>\n","<tr>\n","  <th>Dep. Variable:</th>          <td>medv</td>       <th>  R-squared:         </th> <td>   0.748</td> \n","</tr>\n","<tr>\n","  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.739</td> \n","</tr>\n","<tr>\n","  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   83.38</td> \n","</tr>\n","<tr>\n","  <th>Date:</th>             <td>Tue, 26 Sep 2023</td> <th>  Prob (F-statistic):</th> <td>1.15e-100</td>\n","</tr>\n","<tr>\n","  <th>Time:</th>                 <td>19:09:16</td>     <th>  Log-Likelihood:    </th> <td> -1126.4</td> \n","</tr>\n","<tr>\n","  <th>No. Observations:</th>      <td>   379</td>      <th>  AIC:               </th> <td>   2281.</td> \n","</tr>\n","<tr>\n","  <th>Df Residuals:</th>          <td>   365</td>      <th>  BIC:               </th> <td>   2336.</td> \n","</tr>\n","<tr>\n","  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>     <td> </td>    \n","</tr>\n","<tr>\n","  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n","</tr>\n","<tr>\n","  <th>const</th>   <td>   29.8364</td> <td>    5.861</td> <td>    5.090</td> <td> 0.000</td> <td>   18.310</td> <td>   41.363</td>\n","</tr>\n","<tr>\n","  <th>crim</th>    <td>   -0.1283</td> <td>    0.039</td> <td>   -3.262</td> <td> 0.001</td> <td>   -0.206</td> <td>   -0.051</td>\n","</tr>\n","<tr>\n","  <th>zn</th>      <td>    0.0296</td> <td>    0.017</td> <td>    1.772</td> <td> 0.077</td> <td>   -0.003</td> <td>    0.062</td>\n","</tr>\n","<tr>\n","  <th>indus</th>   <td>    0.0489</td> <td>    0.069</td> <td>    0.706</td> <td> 0.481</td> <td>   -0.087</td> <td>    0.185</td>\n","</tr>\n","<tr>\n","  <th>chas</th>    <td>    2.7735</td> <td>    0.974</td> <td>    2.848</td> <td> 0.005</td> <td>    0.859</td> <td>    4.688</td>\n","</tr>\n","<tr>\n","  <th>nox</th>     <td>  -16.2388</td> <td>    4.432</td> <td>   -3.664</td> <td> 0.000</td> <td>  -24.955</td> <td>   -7.523</td>\n","</tr>\n","<tr>\n","  <th>rm</th>      <td>    4.3688</td> <td>    0.481</td> <td>    9.091</td> <td> 0.000</td> <td>    3.424</td> <td>    5.314</td>\n","</tr>\n","<tr>\n","  <th>age</th>     <td>   -0.0092</td> <td>    0.015</td> <td>   -0.599</td> <td> 0.550</td> <td>   -0.040</td> <td>    0.021</td>\n","</tr>\n","<tr>\n","  <th>dis</th>     <td>   -1.4009</td> <td>    0.237</td> <td>   -5.915</td> <td> 0.000</td> <td>   -1.867</td> <td>   -0.935</td>\n","</tr>\n","<tr>\n","  <th>rad</th>     <td>    0.2578</td> <td>    0.079</td> <td>    3.279</td> <td> 0.001</td> <td>    0.103</td> <td>    0.412</td>\n","</tr>\n","<tr>\n","  <th>tax</th>     <td>   -0.0100</td> <td>    0.004</td> <td>   -2.277</td> <td> 0.023</td> <td>   -0.019</td> <td>   -0.001</td>\n","</tr>\n","<tr>\n","  <th>ptratio</th> <td>   -0.9231</td> <td>    0.150</td> <td>   -6.164</td> <td> 0.000</td> <td>   -1.218</td> <td>   -0.629</td>\n","</tr>\n","<tr>\n","  <th>black</th>   <td>    0.0132</td> <td>    0.003</td> <td>    4.143</td> <td> 0.000</td> <td>    0.007</td> <td>    0.019</td>\n","</tr>\n","<tr>\n","  <th>lstat</th>   <td>   -0.5176</td> <td>    0.058</td> <td>   -8.995</td> <td> 0.000</td> <td>   -0.631</td> <td>   -0.404</td>\n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","  <th>Omnibus:</th>       <td>125.754</td> <th>  Durbin-Watson:     </th> <td>   2.093</td> \n","</tr>\n","<tr>\n","  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 529.968</td> \n","</tr>\n","<tr>\n","  <th>Skew:</th>          <td> 1.392</td>  <th>  Prob(JB):          </th> <td>8.30e-116</td>\n","</tr>\n","<tr>\n","  <th>Kurtosis:</th>      <td> 8.081</td>  <th>  Cond. No.          </th> <td>1.48e+04</td> \n","</tr>\n","</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.48e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."],"text/latex":["\\begin{center}\n","\\begin{tabular}{lclc}\n","\\toprule\n","\\textbf{Dep. Variable:}    &       medv       & \\textbf{  R-squared:         } &     0.748   \\\\\n","\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.739   \\\\\n","\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     83.38   \\\\\n","\\textbf{Date:}             & Tue, 26 Sep 2023 & \\textbf{  Prob (F-statistic):} & 1.15e-100   \\\\\n","\\textbf{Time:}             &     19:09:16     & \\textbf{  Log-Likelihood:    } &   -1126.4   \\\\\n","\\textbf{No. Observations:} &         379      & \\textbf{  AIC:               } &     2281.   \\\\\n","\\textbf{Df Residuals:}     &         365      & \\textbf{  BIC:               } &     2336.   \\\\\n","\\textbf{Df Model:}         &          13      & \\textbf{                     } &             \\\\\n","\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n","\\bottomrule\n","\\end{tabular}\n","\\begin{tabular}{lcccccc}\n","                 & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n","\\midrule\n","\\textbf{const}   &      29.8364  &        5.861     &     5.090  &         0.000        &       18.310    &       41.363     \\\\\n","\\textbf{crim}    &      -0.1283  &        0.039     &    -3.262  &         0.001        &       -0.206    &       -0.051     \\\\\n","\\textbf{zn}      &       0.0296  &        0.017     &     1.772  &         0.077        &       -0.003    &        0.062     \\\\\n","\\textbf{indus}   &       0.0489  &        0.069     &     0.706  &         0.481        &       -0.087    &        0.185     \\\\\n","\\textbf{chas}    &       2.7735  &        0.974     &     2.848  &         0.005        &        0.859    &        4.688     \\\\\n","\\textbf{nox}     &     -16.2388  &        4.432     &    -3.664  &         0.000        &      -24.955    &       -7.523     \\\\\n","\\textbf{rm}      &       4.3688  &        0.481     &     9.091  &         0.000        &        3.424    &        5.314     \\\\\n","\\textbf{age}     &      -0.0092  &        0.015     &    -0.599  &         0.550        &       -0.040    &        0.021     \\\\\n","\\textbf{dis}     &      -1.4009  &        0.237     &    -5.915  &         0.000        &       -1.867    &       -0.935     \\\\\n","\\textbf{rad}     &       0.2578  &        0.079     &     3.279  &         0.001        &        0.103    &        0.412     \\\\\n","\\textbf{tax}     &      -0.0100  &        0.004     &    -2.277  &         0.023        &       -0.019    &       -0.001     \\\\\n","\\textbf{ptratio} &      -0.9231  &        0.150     &    -6.164  &         0.000        &       -1.218    &       -0.629     \\\\\n","\\textbf{black}   &       0.0132  &        0.003     &     4.143  &         0.000        &        0.007    &        0.019     \\\\\n","\\textbf{lstat}   &      -0.5176  &        0.058     &    -8.995  &         0.000        &       -0.631    &       -0.404     \\\\\n","\\bottomrule\n","\\end{tabular}\n","\\begin{tabular}{lclc}\n","\\textbf{Omnibus:}       & 125.754 & \\textbf{  Durbin-Watson:     } &     2.093  \\\\\n","\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &   529.968  \\\\\n","\\textbf{Skew:}          &   1.392 & \\textbf{  Prob(JB):          } & 8.30e-116  \\\\\n","\\textbf{Kurtosis:}      &   8.081 & \\textbf{  Cond. No.          } &  1.48e+04  \\\\\n","\\bottomrule\n","\\end{tabular}\n","%\\caption{OLS Regression Results}\n","\\end{center}\n","\n","Notes: \\newline\n"," [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n"," [2] The condition number is large, 1.48e+04. This might indicate that there are \\newline\n"," strong multicollinearity or other numerical problems."],"text/plain":["<class 'statsmodels.iolib.summary.Summary'>\n","\"\"\"\n","                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                   medv   R-squared:                       0.748\n","Model:                            OLS   Adj. R-squared:                  0.739\n","Method:                 Least Squares   F-statistic:                     83.38\n","Date:                Tue, 26 Sep 2023   Prob (F-statistic):          1.15e-100\n","Time:                        19:09:16   Log-Likelihood:                -1126.4\n","No. Observations:                 379   AIC:                             2281.\n","Df Residuals:                     365   BIC:                             2336.\n","Df Model:                          13                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const         29.8364      5.861      5.090      0.000      18.310      41.363\n","crim          -0.1283      0.039     -3.262      0.001      -0.206      -0.051\n","zn             0.0296      0.017      1.772      0.077      -0.003       0.062\n","indus          0.0489      0.069      0.706      0.481      -0.087       0.185\n","chas           2.7735      0.974      2.848      0.005       0.859       4.688\n","nox          -16.2388      4.432     -3.664      0.000     -24.955      -7.523\n","rm             4.3688      0.481      9.091      0.000       3.424       5.314\n","age           -0.0092      0.015     -0.599      0.550      -0.040       0.021\n","dis           -1.4009      0.237     -5.915      0.000      -1.867      -0.935\n","rad            0.2578      0.079      3.279      0.001       0.103       0.412\n","tax           -0.0100      0.004     -2.277      0.023      -0.019      -0.001\n","ptratio       -0.9231      0.150     -6.164      0.000      -1.218      -0.629\n","black          0.0132      0.003      4.143      0.000       0.007       0.019\n","lstat         -0.5176      0.058     -8.995      0.000      -0.631      -0.404\n","==============================================================================\n","Omnibus:                      125.754   Durbin-Watson:                   2.093\n","Prob(Omnibus):                  0.000   Jarque-Bera (JB):              529.968\n","Skew:                           1.392   Prob(JB):                    8.30e-116\n","Kurtosis:                       8.081   Cond. No.                     1.48e+04\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","[2] The condition number is large, 1.48e+04. This might indicate that there are\n","strong multicollinearity or other numerical problems.\n","\"\"\""]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import statsmodels.api as sm\n","\n","X_train_new = sm.add_constant(X_train)  #have to manually add column of 1s in X data\n","model = sm.OLS(y_train, X_train_new ).fit()   #notice order is different than in scikit-learn, y and then x data\n","\n","model.summary() # get a complete summary of the model, including statistical significance, adjusted Rsquare, F statistic, etc."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"UuVfpqc61ajF"},"source":["# Extra Practice:  At this point, could think about doing some model experimentation by running k-nearest neigbhors on the same data and compare to previous regression results in scikit-learn, perform cross validation and then compare scores.  Could also automate the cross validation we performed above by running gridsearchcv."]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/plain":["182    37.9\n","155    15.6\n","280    45.4\n","126    15.7\n","329    22.6\n","Name: medv, dtype: float64"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["y_train.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Score Comparison: Linear vs. GridSearchCV using KNN (Linear wins)\n","- Try to see if the target variable is categorical (KNN classifier) or numeric (KNN regressor)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["best mean cross-validation score: 0.462\n","best parameters: {'n_neighbors': 7}\n","test-set score: 0.571\n","KNN Grid Search's MSE: 30.04202635384862\n"]}],"source":["# Practice to compare k-nearest neighbors vs. Linear?\n","\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.model_selection import GridSearchCV\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n","\n","\n","# from sklearn import preprocessing\n","# from sklearn import utils\n","# lab = preprocessing.LabelEncoder()\n","# y_train_transformed = lab.fit_transform(y_train)\n","# y_train_transformed = y_train.values.reshape(-1,1)\n","# X_train_transformed = lab.fit_transform(X_train)\n","# y_train_transformed\n","\n","param_grid = {'n_neighbors': [1,3,5,7,9,10,11,13] }#np.arange creates sequence of numbers for each k value\n","\n","grid = GridSearchCV(KNeighborsRegressor(), param_grid=param_grid, cv=10)\n","grid.fit(X_train, y_train)\n","y_pred = grid.predict(X_test)\n","#extract best score and parameter by calling objects \"best_score_\" and \"best_params_\"\n","print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n","print(\"best parameters: {}\".format(grid.best_params_))\n","print(\"test-set score: {:.3f}\".format(grid.score(X_test, y_test)))\n","\n","\n","# MSE\n","from sklearn.metrics import mean_squared_error\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"KNN Grid Search's MSE: {mse}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Score Comparison: Linear vs Random Forest "]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/jeonseo/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n","  warn(\n"]},{"name":"stdout","output_type":"stream","text":["Random Forest's Test-set Score: 0.8518521336172665\n","Random Forest's MSE: 10.374371921259836\n"]}],"source":["from sklearn.ensemble import RandomForestRegressor\n","\n","rf = RandomForestRegressor(n_estimators=100, max_features='auto', random_state=42)\n","\n","# Fit the random forest on the training data\n","rf.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = rf.predict(X_test)\n","\n","score = rf.score(X_test, y_test)\n","\n","# MSE \n","from sklearn.metrics import mean_squared_error\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"Random Forest's Test-set Score: {score}\")\n","print(f\"Random Forest's MSE: {mse}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"nfWdpNs5IeQx"},"source":["## Ridge regression\n","Note: We haven't scaled variables here, but we should.  We'll see how to do this when we learn how to preprocess data."]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HOE7fh2ZIeQ0","outputId":"bcd9f769-af25-4412-e497-1de9b0aa6a4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set score: 0.75\n","Test set score: 0.68\n","MSE: 22.480475501233858\n"]}],"source":["from sklearn.linear_model import Ridge\n","ridge = Ridge().fit(X_train, y_train)\n","print(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\n","print(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))\n","\n","y_pred = ridge.predict(X_test)\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"MSE: {mse}\")"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WGbRjdhtIeQ_","outputId":"97e56e6d-a8ff-4ce9-dcc1-5d78e69b7f82"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set score: 0.74\n","Test set score: 0.67\n","MSE: 22.939228679246213\n"]}],"source":["ridge10 = Ridge(alpha=10).fit(X_train, y_train)\n","print(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\n","print(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))\n","\n","y_pred = ridge10.predict(X_test)\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"MSE: {mse}\")"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKCQ7Jg3IeRL","outputId":"6d30398c-3e2e-4f94-ad44-e15f173f9907"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set score: 0.75\n","Test set score: 0.68\n","MSE: 22.142232974238848\n"]}],"source":["ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\n","print(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\n","print(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))\n","\n","y_pred = ridge01.predict(X_test)\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"MSE: {mse}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VXMNOnXdIeRX"},"source":["## Lasso Regression"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3rlFwSozIeRc","outputId":"e9111974-0432-4e75-bb22-313a9f65c12b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set score: 0.69\n","Test set score: 0.65\n","Number of features used: 10\n","lasso.coef_: [-0.0838981   0.02646051 -0.          0.         -0.          1.54544951\n","  0.01345772 -0.58282853  0.20738089 -0.01121302 -0.70500625  0.01198848\n"," -0.75783702]\n","MSE: 24.39075259035516\n"]}],"source":["from sklearn.linear_model import Lasso\n","from sklearn.linear_model import LassoCV\n","lasso = Lasso().fit(X_train, y_train)\n","lasso = LassoCV(cv=5).fit(X_train, y_train)\n","print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n","print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n","print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))\n","\n","print(\"lasso.coef_: {}\".format(lasso.coef_))\n","\n","\n","y_pred = lasso.predict(X_test)\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"MSE: {mse}\")"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/plain":["10"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["# The totla # of variables that lasso keep it (not removing because it's 0)\n","\n","np.sum(lasso.coef_!=0)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/plain":["3"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["# 3 variables are removed because their respective coefficient reaches to 0. (meaningless variable)\n","np.sum(lasso.coef_==0)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UE_qlatFIeR6","outputId":"cc8af5c7-4f55-4d40-8a3b-5904619d1e2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set score: 0.75\n","Test set score: 0.68\n","Number of features used: 13\n","MSE: 22.21055671270201\n"]}],"source":["\n","# Lower alpha to fit a more complex model\n","# we increase the default setting of \"max_iter\",\n","# otherwise the model would warn us that we should increase max_iter.\n","\n","lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\n","print(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train)))\n","print(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\n","print(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))\n","\n","y_pred = lasso001.predict(X_test)\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"MSE: {mse}\")"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6jRyd33iIeSD","outputId":"75370f10-25b0-4ec5-995b-3dff85ea31a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set score: 0.21\n","Test set score: 0.26\n","Number of features used: 2\n","MSE: 51.95165465021721\n"]}],"source":["# Fit an even more complex model...\n","\n","lasso00001 = Lasso(alpha=100, max_iter=100000).fit(X_train, y_train)\n","print(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train)))\n","print(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test)))\n","print(\"Number of features used: {}\".format(np.sum(lasso00001.coef_ != 0)))\n","\n","y_pred = lasso00001.predict(X_test)\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"MSE: {mse}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"30aLx60RIeSN"},"source":["Questions:\n","Can you tune these models using GridSearchCV?\n","Can you re-run the models using new data? (e.g.- the mtcars dataset, https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1ifDhepFB2mbhslUl2Lnzo_dduBDdzFPV","timestamp":1695670721647}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
